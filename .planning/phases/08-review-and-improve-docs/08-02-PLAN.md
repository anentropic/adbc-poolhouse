---
phase: 08-review-and-improve-docs
plan: "02"
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - docs/src/index.md
  - docs/src/guides/pool-lifecycle.md
  - docs/src/guides/configuration.md
  - docs/src/guides/consumer-patterns.md
autonomous: true
requirements: []

must_haves:
  truths:
    - "index.md explains ADBC driver installation requirement with all warehouse install commands"
    - "index.md lists all typed config class names in the quickstart section"
    - "index.md dispose example uses close_pool(pool) not pool._adbc_source.close()"
    - "pool-lifecycle.md uses close_pool(pool) in all dispose patterns"
    - "pool-lifecycle.md has a Tuning the pool section with kwargs and defaults"
    - "configuration.md has a full kwargs table including pre_ping"
    - "consumer-patterns.md FastAPI example uses close_pool not pool._adbc_source.close()"
    - "no reference to pool._adbc_source appears in any of these four pages"
  artifacts:
    - path: "docs/src/index.md"
      provides: "Updated quickstart with ADBC driver section and config class list"
      contains: "close_pool"
    - path: "docs/src/guides/pool-lifecycle.md"
      provides: "Updated lifecycle guide with new API and tuning section"
      contains: "close_pool"
    - path: "docs/src/guides/configuration.md"
      provides: "Full pool kwargs table including pre_ping"
      contains: "pre_ping"
    - path: "docs/src/guides/consumer-patterns.md"
      provides: "Updated FastAPI example using close_pool"
      contains: "close_pool"
  key_links:
    - from: "docs/src/index.md"
      to: "docs/src/guides/snowflake.md"
      via: "See also link to warehouse guides"
      pattern: "guides/snowflake"
    - from: "docs/src/guides/pool-lifecycle.md"
      to: "close_pool"
      via: "Code examples and prose"
      pattern: "close_pool"
---

<objective>
Update all four existing documentation pages to use the new close_pool/managed_pool
API, add the ADBC driver installation section to index.md, add a config class list,
add a pool tuning section to pool-lifecycle.md, and add a full kwargs table to
configuration.md.

Purpose: Eliminate all pool._adbc_source references from user-facing docs.
Add the two gaps the user identified: ADBC driver install instructions and pool
tuning kwargs documentation.

Output: Four updated markdown files with no private attribute references.
</objective>

<execution_context>
@/Users/paul/.claude/get-shit-done/workflows/execute-plan.md
@/Users/paul/.claude/get-shit-done/templates/summary.md
@.claude/skills/adbc-poolhouse-docs-author/SKILL.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-review-and-improve-docs/08-01-SUMMARY.md
@docs/src/index.md
@docs/src/guides/pool-lifecycle.md
@docs/src/guides/configuration.md
@docs/src/guides/consumer-patterns.md
@docs/src/guides/snowflake.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update index.md — ADBC driver section, config class list, new dispose API</name>
  <files>docs/src/index.md</files>
  <action>
Make three targeted edits to docs/src/index.md:

**Edit 1 — Add ADBC driver section after the Installation section.**

After the pip/uv install code blocks, add a new section:

```markdown
## ADBC drivers

adbc-poolhouse manages the pool, not the driver. You also need an ADBC driver
for your target warehouse. Install the matching extra with adbc-poolhouse:

| Warehouse | Install command |
|---|---|
| DuckDB | `pip install adbc-poolhouse[duckdb]` |
| Snowflake | `pip install adbc-poolhouse[snowflake]` |
| BigQuery | `pip install adbc-poolhouse[bigquery]` |
| PostgreSQL | `pip install adbc-poolhouse[postgresql]` |
| Apache Arrow Flight SQL | `pip install adbc-poolhouse[flightsql]` |
| Databricks | Foundry-distributed — see [Foundry installation](guides/databricks.md) |
| Redshift | Foundry-distributed — see [Foundry installation](guides/redshift.md) |
| Trino | Foundry-distributed — see [Foundry installation](guides/trino.md) |
| MSSQL / Azure SQL / Fabric | Foundry-distributed — see [Foundry installation](guides/mssql.md) |
```

**Edit 2 — Add config class list to the "First pool in five minutes" section.**

After the section heading and before the DuckDB code example, add:

```markdown
All supported warehouses have a typed config class:
`DuckDBConfig`, `SnowflakeConfig`, `BigQueryConfig`, `PostgreSQLConfig`,
`FlightSQLConfig`, `DatabricksConfig`, `RedshiftConfig`, `TrinoConfig`,
`MSSQLConfig`.

```

**Edit 3 — Replace the dispose pattern and prose in the code example.**

Replace:
```python
pool.dispose()
pool._adbc_source.close()
```
with:
```python
close_pool(pool)
```

Update the import line to include `close_pool`:
```python
from adbc_poolhouse import DuckDBConfig, create_pool, close_pool
```

Replace the prose that says:
"`pool.dispose()` drains the pool; `pool._adbc_source.close()` releases the underlying ADBC source connection that the pool holds internally."

With:
"`pool.connect()` checks out a connection from the pool and returns it when the `with` block exits. `close_pool(pool)` drains the pool and closes the underlying ADBC source connection."

**Edit 4 — Update What's next links.**

Add warehouse guide entries to the What's next list. Add after the Snowflake guide link:
```markdown
- [Warehouse guides](guides/duckdb.md) — per-warehouse install commands, auth examples, and env var prefixes
```

**Voice:** terse, technical, no hand-holding. Apply humanizer pass to all rewritten prose.
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && grep -c "_adbc_source" docs/src/index.md && echo "count above should be 0"</automated>
  </verify>
  <done>index.md has ADBC driver section with table, config class list, close_pool in dispose example, no _adbc_source references</done>
</task>

<task type="auto">
  <name>Task 2: Update pool-lifecycle.md — new API and pool tuning section</name>
  <files>docs/src/guides/pool-lifecycle.md</files>
  <action>
Make three targeted edits to docs/src/guides/pool-lifecycle.md:

**Edit 1 — Replace all pool._adbc_source references with close_pool.**

The "Disposing the pool" section currently shows:
```python
pool.dispose()
pool._adbc_source.close()
```
Replace with:
```python
from adbc_poolhouse import close_pool

close_pool(pool)
```

Rewrite the prose in that section:
OLD: "`pool.dispose()` drains the pool and closes each pooled connection. The ADBC source connection that the pool was built from is a separate object — it stays open until you close it explicitly. Skipping `pool._adbc_source.close()` leaves a file handle or network socket open until the process exits."

NEW:
"`close_pool` drains the pool, closes each pooled connection, and releases the ADBC source connection in one call. Calling `pool.dispose()` alone leaves a file handle or network socket open until the process exits."

**Edit 2 — Update the pytest fixture example.**

Replace:
```python
    p.dispose()
    p._adbc_source.close()
```
With:
```python
    from adbc_poolhouse import close_pool
    close_pool(p)
```

Update the import line in the fixture if needed: `from adbc_poolhouse import DuckDBConfig, create_pool`

Also add a `managed_pool` alternative after the fixture example:

```markdown
For scripts and short-lived processes, use `managed_pool` as a context manager instead:

```python
from adbc_poolhouse import DuckDBConfig, managed_pool

with managed_pool(DuckDBConfig(database="/tmp/test.db")) as pool:
    with pool.connect() as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT 1")
# pool is automatically closed when the with block exits
```
```

**Edit 3 — Add "Tuning the pool" section before See also.**

Add this section:

```markdown
## Tuning the pool

`create_pool` (and `managed_pool`) accept keyword arguments to tune pool behaviour.
The defaults are conservative and appropriate for most use cases:

| Argument | Default | Description |
|---|---|---|
| `pool_size` | `5` | Connections kept in the pool at all times (DuckDB defaults to `1`) |
| `max_overflow` | `3` | Extra connections allowed above `pool_size` when demand is high |
| `timeout` | `30` | Seconds to wait for a connection before raising `TimeoutError` |
| `recycle` | `3600` | Seconds before a connection is closed and replaced |
| `pre_ping` | `False` | Ping connections before checkout (disabled — does not function on standalone `QueuePool` without a SQLAlchemy dialect; use `recycle` instead) |

Pass any of these to `create_pool`:

```python
pool = create_pool(config, pool_size=10, recycle=7200)
```
```

**Update "Common mistakes" section:**

The "Not closing `_adbc_source` after `dispose()`" section must be rewritten.
The mistake is now calling `pool.dispose()` directly instead of `close_pool()`:

```markdown
**Calling `pool.dispose()` without `close_pool()`**

`pool.dispose()` drains the pool but does not close the ADBC source connection.
Always use `close_pool(pool)` (or `managed_pool` as a context manager) — never
call `pool.dispose()` directly.
```

**Voice:** terse, technical, no hand-holding. Apply humanizer pass.
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && grep -c "_adbc_source" docs/src/guides/pool-lifecycle.md && echo "count above should be 0"</automated>
  </verify>
  <done>pool-lifecycle.md uses close_pool throughout; no _adbc_source references; Tuning the pool section present with kwargs table</done>
</task>

<task type="auto">
  <name>Task 3: Update configuration.md (pre_ping row) and consumer-patterns.md (close_pool)</name>
  <files>docs/src/guides/configuration.md, docs/src/guides/consumer-patterns.md</files>
  <action>
**configuration.md — add pre_ping row to the Pool tuning table.**

The existing Pool tuning table has four rows (pool_size, max_overflow, timeout, recycle).
Add a fifth row for pre_ping:

| `pre_ping` | `False` | Ping connections before checkout. Disabled by default — does not function on standalone `QueuePool` without a SQLAlchemy dialect; use `recycle` for connection health. |

Also update the existing pool_size row description to match POOL-02 spec: "Number of connections to keep open (DuckDB defaults to `1`)" — this is already correct in the current file. Verify `max_overflow` default is `10` in the table. Note: the `create_pool` function default for max_overflow is `3`, but configuration.md's table says `10` — align with the actual source code default of `3`.

Check the current table and correct the max_overflow default if it says 10 (it should match the code: 3).

**consumer-patterns.md — replace _adbc_source reference in FastAPI example.**

Find the FastAPI lifespan example:
```python
    pool.dispose()
    pool._adbc_source.close()
```

Replace with:
```python
    close_pool(pool)
```

Update the import line:
```python
from adbc_poolhouse import DuckDBConfig, create_pool
```
Change to:
```python
from adbc_poolhouse import DuckDBConfig, create_pool, close_pool
```

No other changes needed to consumer-patterns.md — the dbt section was already
updated during the discuss-phase (Profile.from_raw_profiles pattern) and does
not reference _adbc_source.

After both edits, verify no _adbc_source references remain in either file.
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && grep -rn "_adbc_source" docs/src/ | grep -v "^Binary" && echo "above should be empty"</automated>
  </verify>
  <done>configuration.md has pre_ping row and correct max_overflow default; consumer-patterns.md FastAPI example uses close_pool; no _adbc_source in any docs/src/ file; uv run mkdocs build --strict passes</done>
</task>

</tasks>

<verification>
```bash
cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && \
  grep -rn "_adbc_source" docs/src/ | wc -l && \
  echo "count above must be 0" && \
  uv run mkdocs build --strict 2>&1 | tail -5
```
</verification>

<success_criteria>
- Zero occurrences of _adbc_source in docs/src/ directory
- index.md: ADBC driver table present, config class list present, close_pool in quickstart
- pool-lifecycle.md: close_pool throughout, Tuning the pool section with 5-row kwargs table
- configuration.md: pre_ping row in pool tuning table, max_overflow shows 3 (matching code)
- consumer-patterns.md: FastAPI example uses close_pool
- uv run mkdocs build --strict exits 0
</success_criteria>

<output>
After completion, create `.planning/phases/08-review-and-improve-docs/08-02-SUMMARY.md`
</output>

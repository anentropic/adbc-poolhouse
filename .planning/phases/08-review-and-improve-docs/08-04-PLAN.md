---
phase: 08-review-and-improve-docs
plan: "04"
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/src/guides/duckdb.md
  - docs/src/guides/bigquery.md
  - docs/src/guides/postgresql.md
  - docs/src/guides/flightsql.md
autonomous: true
requirements: []

must_haves:
  truths:
    - "DuckDB guide page exists following the Snowflake guide structure"
    - "BigQuery guide page exists following the Snowflake guide structure"
    - "PostgreSQL guide page exists following the Snowflake guide structure"
    - "FlightSQL guide page exists following the Snowflake guide structure"
    - "All four pages use pip install adbc-poolhouse[extra] install command"
    - "All four pages have a Loading from environment variables section"
    - "All four pages have a See also section"
  artifacts:
    - path: "docs/src/guides/duckdb.md"
      provides: "DuckDB warehouse guide"
      contains: "DUCKDB_"
    - path: "docs/src/guides/bigquery.md"
      provides: "BigQuery warehouse guide"
      contains: "BIGQUERY_"
    - path: "docs/src/guides/postgresql.md"
      provides: "PostgreSQL warehouse guide"
      contains: "POSTGRESQL_"
    - path: "docs/src/guides/flightsql.md"
      provides: "FlightSQL warehouse guide"
      contains: "FLIGHTSQL_"
  key_links:
    - from: "docs/src/guides/duckdb.md"
      to: "docs/src/guides/configuration.md"
      via: "See also link"
      pattern: "configuration\\.md"
    - from: "docs/src/guides/bigquery.md"
      to: "docs/src/guides/configuration.md"
      via: "See also link"
      pattern: "configuration\\.md"
---

<objective>
Create four new per-warehouse guide pages for PyPI-distributed warehouses: DuckDB,
BigQuery, PostgreSQL, and FlightSQL. Each follows the existing Snowflake guide
structure exactly.

Purpose: Users selecting a warehouse need a dedicated page covering install,
auth methods, env var prefix, and cross-links. The Snowflake guide is the
established pattern — these four pages replicate that structure for the remaining
PyPI-distributed warehouses.

Output: Four new markdown files in docs/src/guides/.
</objective>

<execution_context>
@/Users/paul/.claude/get-shit-done/workflows/execute-plan.md
@/Users/paul/.claude/get-shit-done/templates/summary.md
@.claude/skills/adbc-poolhouse-docs-author/SKILL.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@docs/src/guides/snowflake.md
@src/adbc_poolhouse/_duckdb_config.py
@src/adbc_poolhouse/_bigquery_config.py
@src/adbc_poolhouse/_postgresql_config.py
@src/adbc_poolhouse/_flightsql_config.py

<interfaces>
<!-- Key fields per config class — extracted from source for executor reference -->

DuckDBConfig (env_prefix="DUCKDB_"):
  database: str = ":memory:"   # file path or :memory:
  read_only: bool = False       # DUCKDB_READ_ONLY
  config: dict | None = None    # ADBC config options
  NOTE: pool_size=1 is enforced for :memory: databases; use file path for pool_size > 1

BigQueryConfig (env_prefix="BIGQUERY_"):
  auth_type: str | None = None          # BIGQUERY_AUTH_TYPE
  # auth_type values: 'default', 'json_credential_file', 'json_credential_string', 'user_authentication'
  auth_credentials_path: str | None = None  # BIGQUERY_AUTH_CREDENTIALS_PATH (JSON key file path)
  auth_client_id: str | None = None         # BIGQUERY_AUTH_CLIENT_ID
  auth_client_secret: SecretStr | None = None  # BIGQUERY_AUTH_CLIENT_SECRET
  auth_refresh_token: SecretStr | None = None  # BIGQUERY_AUTH_REFRESH_TOKEN
  project_id: str | None = None           # BIGQUERY_PROJECT_ID
  dataset_id: str | None = None           # BIGQUERY_DATASET_ID

PostgreSQLConfig (env_prefix="POSTGRESQL_"):
  uri: str | None = None   # POSTGRESQL_URI
  # Format: postgresql://user:password@host:5432/dbname?sslmode=require

FlightSQLConfig (env_prefix="FLIGHTSQL_"):
  uri: str | None = None           # FLIGHTSQL_URI — grpc://host:port or grpc+tls://host:port
  username: str | None = None      # FLIGHTSQL_USERNAME
  password: SecretStr | None = None  # FLIGHTSQL_PASSWORD
  authorization_header: SecretStr | None = None  # FLIGHTSQL_AUTHORIZATION_HEADER
  tls_skip_verify: bool = False    # FLIGHTSQL_TLS_SKIP_VERIFY
  tls_root_certs: str | None = None  # FLIGHTSQL_TLS_ROOT_CERTS
  with_cookie_middleware: bool = False   # FLIGHTSQL_WITH_COOKIE_MIDDLEWARE
  tls_override_hostname: str | None = None  # FLIGHTSQL_TLS_OVERRIDE_HOSTNAME
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create docs/src/guides/duckdb.md</name>
  <files>docs/src/guides/duckdb.md</files>
  <action>
Create the DuckDB warehouse guide. Follow the snowflake.md structure exactly:
install section, connection section with code examples, env var section, See also.

Key points to cover:
- Install: `pip install adbc-poolhouse[duckdb]` (or uv add)
- File-backed vs in-memory: DuckDB defaults to `:memory:` but pool_size is capped at 1
  for in-memory; use a file path for multi-connection pools
- Read-only mode: set `read_only=True` (env: `DUCKDB_READ_ONLY=true`)
- Env prefix: `DUCKDB_`
- See also: configuration.md, pool-lifecycle.md

Example structure:

```markdown
# DuckDB guide

Install the DuckDB extra:

```bash
pip install adbc-poolhouse[duckdb]
```

Or with uv:

```bash
uv add "adbc-poolhouse[duckdb]"
```

## Connection

`DuckDBConfig` supports file-backed and in-memory databases. Use a file path
for multi-connection pools — in-memory databases are connection-isolated.

### File-backed

```python
from adbc_poolhouse import DuckDBConfig, create_pool

config = DuckDBConfig(database="/tmp/warehouse.db")
pool = create_pool(config)
```

### In-memory (single connection)

`pool_size` is forced to `1` for in-memory databases. Each connection would
otherwise get its own isolated empty database, which is almost always a bug.

```python
config = DuckDBConfig(database=":memory:")
pool = create_pool(config)
```

### Read-only

```python
config = DuckDBConfig(database="/data/warehouse.db", read_only=True)
pool = create_pool(config)
```

## Loading from environment variables

`DuckDBConfig` reads all fields from environment variables with the `DUCKDB_` prefix:

```bash
export DUCKDB_DATABASE=/data/warehouse.db
export DUCKDB_READ_ONLY=false
```

```python
config = DuckDBConfig()  # reads from env
```

## See also

- [Configuration reference](configuration.md) — env_prefix, pool tuning
- [Pool lifecycle](pool-lifecycle.md) — close_pool, pytest fixtures
```

Voice: terse, technical. Apply humanizer pass to all prose.
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && test -f docs/src/guides/duckdb.md && grep -c "DUCKDB_" docs/src/guides/duckdb.md</automated>
  </verify>
  <done>docs/src/guides/duckdb.md exists with install section, connection examples (file/in-memory/read-only), env var section, See also</done>
</task>

<task type="auto">
  <name>Task 2: Create docs/src/guides/bigquery.md, postgresql.md, flightsql.md</name>
  <files>docs/src/guides/bigquery.md, docs/src/guides/postgresql.md, docs/src/guides/flightsql.md</files>
  <action>
Create three more warehouse guide pages following the Snowflake guide structure.

**docs/src/guides/bigquery.md:**

```markdown
# BigQuery guide

Install the BigQuery extra:

```bash
pip install adbc-poolhouse[bigquery]
```

Or with uv:

```bash
uv add "adbc-poolhouse[bigquery]"
```

## Auth methods

`BigQueryConfig` supports four auth methods via `auth_type`:

### Application Default Credentials (default)

If `auth_type` is unset, the driver uses Google ADC — the same credentials
`gcloud auth application-default login` or a service account key file in
`GOOGLE_APPLICATION_CREDENTIALS` provides.

```python
from adbc_poolhouse import BigQueryConfig, create_pool

config = BigQueryConfig(project_id="my-gcp-project")
pool = create_pool(config)
```

### JSON credential file

```python
config = BigQueryConfig(
    auth_type="json_credential_file",
    auth_credentials_path="/keys/service_account.json",
    project_id="my-gcp-project",
)
```

### User authentication (OAuth)

```python
config = BigQueryConfig(
    auth_type="user_authentication",
    auth_client_id="...",
    auth_client_secret="...",
    auth_refresh_token="...",
    project_id="my-gcp-project",
)
```

## Loading from environment variables

`BigQueryConfig` reads all fields from environment variables with the `BIGQUERY_` prefix:

```bash
export BIGQUERY_PROJECT_ID=my-gcp-project
export BIGQUERY_DATASET_ID=my_dataset
```

```python
config = BigQueryConfig()  # reads from env
```

## See also

- [Configuration reference](configuration.md) — env_prefix, pool tuning
- [Consumer patterns](consumer-patterns.md) — FastAPI and dbt examples
```

---

**docs/src/guides/postgresql.md:**

PostgreSQLConfig is URI-only — all connection parameters are embedded in the URI.

```markdown
# PostgreSQL guide

Install the PostgreSQL extra:

```bash
pip install adbc-poolhouse[postgresql]
```

Or with uv:

```bash
uv add "adbc-poolhouse[postgresql]"
```

## Connection

`PostgreSQLConfig` takes a single `uri` field. All connection parameters —
host, port, user, password, database, SSL mode — are embedded in the URI.

```python
from adbc_poolhouse import PostgreSQLConfig, create_pool

config = PostgreSQLConfig(
    uri="postgresql://me:s3cret@db.example.com:5432/mydb?sslmode=require",  # pragma: allowlist secret
)
pool = create_pool(config)
```

## Loading from environment variables

`PostgreSQLConfig` reads fields from environment variables with the `POSTGRESQL_` prefix:

```bash
export POSTGRESQL_URI=postgresql://me:s3cret@db.example.com:5432/mydb
```

```python
config = PostgreSQLConfig()  # reads from env
```

## See also

- [Configuration reference](configuration.md) — env_prefix, pool tuning
- [Pool lifecycle](pool-lifecycle.md) — close_pool, pytest fixtures
```

---

**docs/src/guides/flightsql.md:**

FlightSQLConfig supports gRPC plaintext and TLS connections with username/password
or a raw authorization header.

```markdown
# Apache Arrow Flight SQL guide

Install the Flight SQL extra:

```bash
pip install adbc-poolhouse[flightsql]
```

Or with uv:

```bash
uv add "adbc-poolhouse[flightsql]"
```

## Connection

`FlightSQLConfig` connects to any Arrow Flight SQL server. Set `uri` to the
gRPC endpoint and provide credentials via username/password or a raw
authorization header.

### Username and password (plaintext)

```python
from adbc_poolhouse import FlightSQLConfig, create_pool

config = FlightSQLConfig(
    uri="grpc://localhost:32010",
    username="me",
    password="s3cret",  # pragma: allowlist secret
)
pool = create_pool(config)
```

### TLS with certificate verification

```python
config = FlightSQLConfig(
    uri="grpc+tls://db.example.com:443",
    username="me",
    password="s3cret",  # pragma: allowlist secret
    tls_root_certs="/path/to/ca.pem",
)
```

### Raw authorization header

Use `authorization_header` when the server expects a pre-formatted token
(e.g. `Bearer eyJ...` or `Basic base64==`):

```python
config = FlightSQLConfig(
    uri="grpc+tls://db.example.com:443",
    authorization_header="Bearer eyJ...",
)
```

## Loading from environment variables

`FlightSQLConfig` reads all fields from environment variables with the `FLIGHTSQL_` prefix:

```bash
export FLIGHTSQL_URI=grpc+tls://db.example.com:443
export FLIGHTSQL_USERNAME=me
export FLIGHTSQL_PASSWORD=s3cret
```

```python
config = FlightSQLConfig()  # reads from env
```

## See also

- [Configuration reference](configuration.md) — env_prefix, pool tuning
- [Pool lifecycle](pool-lifecycle.md) — close_pool, pytest fixtures
```

Voice: terse, technical. Apply humanizer pass to all prose after writing.
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && test -f docs/src/guides/bigquery.md && test -f docs/src/guides/postgresql.md && test -f docs/src/guides/flightsql.md && echo "all three files exist"</automated>
  </verify>
  <done>bigquery.md, postgresql.md, flightsql.md exist; each has install section, connection examples, env var section with correct prefix, See also; no _adbc_source references</done>
</task>

</tasks>

<verification>
```bash
cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && \
  for f in docs/src/guides/duckdb.md docs/src/guides/bigquery.md docs/src/guides/postgresql.md docs/src/guides/flightsql.md; do
    test -f "$f" && echo "$f: OK" || echo "$f: MISSING"
  done
```

Note: Full mkdocs build verification is in Plan 06 after all guides exist and nav is updated.
</verification>

<success_criteria>
- Four guide pages exist: duckdb.md, bigquery.md, postgresql.md, flightsql.md
- Each has install section with pip and uv commands
- Each has connection/auth section with code examples using the correct config class
- Each has Loading from environment variables section with correct env prefix
- Each has See also section linking to configuration.md and pool-lifecycle.md
- No reference to pool._adbc_source in any of these pages
</success_criteria>

<output>
After completion, create `.planning/phases/08-review-and-improve-docs/08-04-SUMMARY.md`
</output>

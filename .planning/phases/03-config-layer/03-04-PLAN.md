---
phase: 03-config-layer
plan: 04
type: execute
wave: 2
depends_on:
  - 03-01
files_modified:
  - src/adbc_poolhouse/_databricks_config.py
  - src/adbc_poolhouse/_redshift_config.py
  - src/adbc_poolhouse/_trino_config.py
autonomous: true
requirements:
  - CFG-06

must_haves:
  truths:
    - "DatabricksConfig() constructs with all-optional fields"
    - "RedshiftConfig(uri='redshift://host/db') constructs successfully"
    - "TrinoConfig(uri='trino://host:8080/catalog') constructs successfully"
    - "DATABRICKS_TOKEN env var populates DatabricksConfig.token as SecretStr"
    - "REDSHIFT_AWS_SECRET_ACCESS_KEY env var populates as SecretStr"
    - "TRINO_PASSWORD env var populates as SecretStr"
    - "All three configs are instances of WarehouseConfig Protocol"
  artifacts:
    - path: "src/adbc_poolhouse/_databricks_config.py"
      provides: "DatabricksConfig for Columnar ADBC Databricks driver"
      exports: ["DatabricksConfig"]
    - path: "src/adbc_poolhouse/_redshift_config.py"
      provides: "RedshiftConfig for Columnar ADBC Redshift driver"
      exports: ["RedshiftConfig"]
    - path: "src/adbc_poolhouse/_trino_config.py"
      provides: "TrinoConfig for Columnar ADBC Trino driver"
      exports: ["TrinoConfig"]
  key_links:
    - from: "src/adbc_poolhouse/_databricks_config.py"
      to: "src/adbc_poolhouse/_base_config.py"
      via: "class DatabricksConfig(BaseWarehouseConfig)"
      pattern: "class DatabricksConfig.*BaseWarehouseConfig"
    - from: "src/adbc_poolhouse/_redshift_config.py"
      to: "src/adbc_poolhouse/_base_config.py"
      via: "class RedshiftConfig(BaseWarehouseConfig)"
      pattern: "class RedshiftConfig.*BaseWarehouseConfig"
    - from: "src/adbc_poolhouse/_trino_config.py"
      to: "src/adbc_poolhouse/_base_config.py"
      via: "class TrinoConfig(BaseWarehouseConfig)"
      pattern: "class TrinoConfig.*BaseWarehouseConfig"
---

<objective>
Implement Foundry-distributed backend configs for Databricks, Redshift, and Trino.

Purpose: These three use URI-primary designs (like PostgreSQL) plus decomposed auth fields for use cases where building a full URI is inconvenient. All Foundry drivers are not on PyPI — these configs must be constructible with only pydantic-settings, no driver import.
Output: _databricks_config.py, _redshift_config.py, _trino_config.py
</objective>

<execution_context>
@/Users/paul/.claude/get-shit-done/workflows/execute-plan.md
@/Users/paul/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-config-layer/03-RESEARCH.md
@.planning/phases/03-config-layer/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create _databricks_config.py and _redshift_config.py</name>
  <files>src/adbc_poolhouse/_databricks_config.py, src/adbc_poolhouse/_redshift_config.py</files>
  <action>
**DatabricksConfig** — `src/adbc_poolhouse/_databricks_config.py`:

Confidence: MEDIUM (URI format verified from docs.adbc-drivers.org; host/token/http_path decomposition triangulated from Databricks ecosystem conventions).

```python
from __future__ import annotations

from pydantic import Field, SecretStr
from pydantic_settings import SettingsConfigDict

from adbc_poolhouse._base_config import BaseWarehouseConfig


class DatabricksConfig(BaseWarehouseConfig):
    """Databricks warehouse configuration.

    Uses the Columnar ADBC Databricks driver (Foundry-distributed, not on
    PyPI). Install via the ADBC Driver Foundry.

    Supports PAT (personal access token) and OAuth (U2M and M2M) auth.
    Connection can be specified as a full URI or via decomposed fields.

    Pool tuning fields are inherited and loaded from DATABRICKS_* env vars.

    Note: This driver is distributed via the ADBC Driver Foundry, not PyPI.
    See project Phase 7 documentation for Foundry installation instructions.
    """

    model_config = SettingsConfigDict(env_prefix='DATABRICKS_')

    uri: SecretStr | None = None
    """Full connection URI: databricks://token:<token>@<host>:443/<http-path>
    May contain credentials — stored as SecretStr. Env: DATABRICKS_URI."""

    host: str | None = None
    """Databricks workspace hostname (e.g. 'adb-xxx.azuredatabricks.net').
    Alternative to embedding host in URI. Env: DATABRICKS_HOST."""

    http_path: str | None = None
    """SQL warehouse HTTP path (e.g. '/sql/1.0/warehouses/abc123').
    Env: DATABRICKS_HTTP_PATH."""

    token: SecretStr | None = None
    """Personal access token for PAT auth. Env: DATABRICKS_TOKEN."""

    auth_type: str | None = None
    """OAuth auth type: 'OAuthU2M' (browser-based) or 'OAuthM2M' (service
    principal). Omit for PAT auth. Env: DATABRICKS_AUTH_TYPE."""

    client_id: str | None = None
    """OAuth M2M service principal client ID. Env: DATABRICKS_CLIENT_ID."""

    client_secret: SecretStr | None = None
    """OAuth M2M service principal client secret. Env: DATABRICKS_CLIENT_SECRET."""

    catalog: str | None = None
    """Default Unity Catalog. Env: DATABRICKS_CATALOG."""

    schema_: str | None = Field(default=None, validation_alias='schema', alias='schema')
    """Default schema. Python attribute is schema_ to avoid Pydantic conflicts.
    Env: DATABRICKS_SCHEMA."""

    def _adbc_driver_key(self) -> str:
        return 'databricks'
```

**RedshiftConfig** — `src/adbc_poolhouse/_redshift_config.py`:

Confidence: HIGH (docs.adbc-drivers.org/drivers/redshift is live and documented).

```python
from __future__ import annotations

from pydantic import SecretStr
from pydantic_settings import SettingsConfigDict

from adbc_poolhouse._base_config import BaseWarehouseConfig


class RedshiftConfig(BaseWarehouseConfig):
    """Redshift warehouse configuration.

    Uses the Columnar ADBC Redshift driver (Foundry-distributed, not on
    PyPI). Supports provisioned clusters (standard and IAM auth) and
    Redshift Serverless.

    Pool tuning fields are inherited and loaded from REDSHIFT_* env vars.

    Note: This driver is distributed via the ADBC Driver Foundry, not PyPI.
    See project Phase 7 documentation for Foundry installation instructions.
    """

    model_config = SettingsConfigDict(env_prefix='REDSHIFT_')

    uri: str | None = None
    """Connection URI: redshift://[user:password@]host[:port]/dbname[?params]
    Use redshift:///dbname for automatic endpoint discovery.
    Env: REDSHIFT_URI."""

    cluster_type: str | None = None
    """Cluster variant: 'redshift' (standard), 'redshift-iam', or
    'redshift-serverless'. Env: REDSHIFT_CLUSTER_TYPE."""

    cluster_identifier: str | None = None
    """Provisioned cluster identifier (required for IAM auth).
    Env: REDSHIFT_CLUSTER_IDENTIFIER."""

    workgroup_name: str | None = None
    """Serverless workgroup name. Env: REDSHIFT_WORKGROUP_NAME."""

    aws_region: str | None = None
    """AWS region (e.g. 'us-west-2'). Env: REDSHIFT_AWS_REGION."""

    aws_access_key_id: str | None = None
    """AWS IAM access key ID. Env: REDSHIFT_AWS_ACCESS_KEY_ID."""

    aws_secret_access_key: SecretStr | None = None
    """AWS IAM secret access key. Env: REDSHIFT_AWS_SECRET_ACCESS_KEY."""

    sslmode: str | None = None
    """SSL mode (e.g. 'require', 'verify-full'). Env: REDSHIFT_SSLMODE."""

    def _adbc_driver_key(self) -> str:
        return 'redshift'
```

**Implementation notes:**
- `DatabricksConfig.uri` is `SecretStr` (may contain token in URI path).
- `DatabricksConfig.schema_` uses `Field(validation_alias='schema', alias='schema')` — same pattern as SnowflakeConfig (see 03-02-SUMMARY for resolution). If that approach changed, use the same resolution here.
- `RedshiftConfig.uri` is plain `str` (no credential embedding — IAM auth uses separate fields).
- No cross-field validators for either model.
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && uv run python -c "
from adbc_poolhouse._databricks_config import DatabricksConfig
from adbc_poolhouse._redshift_config import RedshiftConfig
from adbc_poolhouse._base_config import WarehouseConfig
from pydantic import SecretStr

db = DatabricksConfig()
assert db.pool_size == 5
assert db.token is None
assert isinstance(db, WarehouseConfig)
print('PASS: DatabricksConfig defaults')

import os
os.environ['DATABRICKS_TOKEN'] = 'dapi123'
db2 = DatabricksConfig()
assert isinstance(db2.token, SecretStr)
assert 'dapi123' not in repr(db2)
del os.environ['DATABRICKS_TOKEN']
print('PASS: DatabricksConfig token is SecretStr')

rs = RedshiftConfig()
assert rs.pool_size == 5
assert rs.uri is None
assert isinstance(rs, WarehouseConfig)
print('PASS: RedshiftConfig defaults')
"</automated>
  </verify>
  <done>DatabricksConfig and RedshiftConfig exist; both construct with no required args; token and aws_secret_access_key are SecretStr; both are WarehouseConfig Protocol instances.</done>
</task>

<task type="auto">
  <name>Task 2: Create _trino_config.py</name>
  <files>src/adbc_poolhouse/_trino_config.py</files>
  <action>
Create `src/adbc_poolhouse/_trino_config.py`:

Confidence: HIGH (docs.adbc-drivers.org/drivers/trino is live and documented).

```python
from __future__ import annotations

from pydantic import Field, SecretStr
from pydantic_settings import SettingsConfigDict

from adbc_poolhouse._base_config import BaseWarehouseConfig


class TrinoConfig(BaseWarehouseConfig):
    """Trino warehouse configuration.

    Uses the Columnar ADBC Trino driver (Foundry-distributed, not on PyPI).
    Supports URI-based or decomposed field connection specification.

    Pool tuning fields are inherited and loaded from TRINO_* env vars.

    Note: This driver is distributed via the ADBC Driver Foundry, not PyPI.
    See project Phase 7 documentation for Foundry installation instructions.
    """

    model_config = SettingsConfigDict(env_prefix='TRINO_')

    uri: str | None = None
    """Connection URI: trino://[user[:password]@]host[:port][/catalog[/schema]][?params]
    Env: TRINO_URI."""

    host: str | None = None
    """Trino coordinator hostname. Alternative to URI. Env: TRINO_HOST."""

    port: int | None = None
    """Trino coordinator port. Defaults: 8080 (HTTP), 8443 (HTTPS).
    Env: TRINO_PORT."""

    user: str | None = None
    """Username. Env: TRINO_USER."""

    password: SecretStr | None = None
    """Password (HTTPS connections only). Env: TRINO_PASSWORD."""

    catalog: str | None = None
    """Default catalog. Env: TRINO_CATALOG."""

    schema_: str | None = Field(default=None, validation_alias='schema', alias='schema')
    """Default schema. Python attribute is schema_ to avoid Pydantic conflicts.
    Env: TRINO_SCHEMA."""

    ssl: bool = True
    """Use HTTPS. Disable for local development clusters. Env: TRINO_SSL."""

    ssl_verify: bool = True
    """Verify SSL certificate. Env: TRINO_SSL_VERIFY."""

    source: str | None = None
    """Application identifier sent to Trino coordinator.
    Env: TRINO_SOURCE."""

    def _adbc_driver_key(self) -> str:
        return 'trino'
```

**Implementation notes:**
- `schema_` uses same `Field(validation_alias='schema', alias='schema')` pattern as DatabricksConfig. Reference 03-02-SUMMARY for any schema field resolution notes.
- `ssl: bool = True` — Trino defaults to SSL enabled. This matches the Columnar driver default and is the safe default for production.
- `password` is `SecretStr` even though it is HTTPS-only in practice.
- No cross-field validators (ssl + password combination is not validated here — caller responsibility).
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && uv run python -c "
from adbc_poolhouse._trino_config import TrinoConfig
from adbc_poolhouse._base_config import WarehouseConfig

t = TrinoConfig()
assert t.pool_size == 5
assert t.ssl is True
assert t.ssl_verify is True
assert t.uri is None
assert isinstance(t, WarehouseConfig)
print('PASS: TrinoConfig defaults')

import os
os.environ['TRINO_URI'] = 'trino://host:8080/catalog'
t2 = TrinoConfig()
assert t2.uri == 'trino://host:8080/catalog'
del os.environ['TRINO_URI']
print('PASS: TrinoConfig URI env var')
"</automated>
  </verify>
  <done>TrinoConfig exists with all fields; ssl defaults True; ssl_verify defaults True; TRINO_URI env var loads; isinstance WarehouseConfig Protocol.</done>
</task>

</tasks>

<verification>
```bash
cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse
uv run python -c "
from adbc_poolhouse._databricks_config import DatabricksConfig
from adbc_poolhouse._redshift_config import RedshiftConfig
from adbc_poolhouse._trino_config import TrinoConfig
from adbc_poolhouse._base_config import WarehouseConfig

for cls in [DatabricksConfig, RedshiftConfig, TrinoConfig]:
    obj = cls()
    assert isinstance(obj, WarehouseConfig), f'{cls.__name__} is not WarehouseConfig'
    assert obj.pool_size == 5
print('PASS: All three Foundry (Part 1) configs valid')
"
uv run prek 2>&1 | tail -5
```
</verification>

<success_criteria>
- `_databricks_config.py`, `_redshift_config.py`, `_trino_config.py` all exist
- Each constructs with no required args
- Sensitive fields (token, aws_secret_access_key, password) are SecretStr
- All three are instances of WarehouseConfig Protocol
- schema_ field naming follows same resolution as SnowflakeConfig (see 03-02-SUMMARY)
- prek passes
</success_criteria>

<output>
After completion, create `.planning/phases/03-config-layer/03-04-SUMMARY.md` with:
- Files created and key field decisions
- schema_ alias resolution (consistent with 03-02-SUMMARY)
- Confidence notes for Databricks field decomposition
</output>

---
phase: 05-pool-factory-and-duckdb-integration
plan: "02"
type: tdd
wave: 2
depends_on:
  - "05-01"
files_modified:
  - tests/test_pool_factory.py
  - src/adbc_poolhouse/_pool_factory.py
  - src/adbc_poolhouse/__init__.py
autonomous: true
requirements:
  - POOL-01
  - POOL-02
  - POOL-03
  - POOL-04
  - POOL-05
  - TEST-01
  - TEST-07

must_haves:
  truths:
    - "create_pool(DuckDBConfig(database='/tmp/test.db')) returns a sqlalchemy.pool.QueuePool"
    - "Default pool settings are pool_size=5, max_overflow=3, timeout=30, pre_ping=False, recycle=3600"
    - "create_pool(cfg, pool_size=10, recycle=7200) overrides defaults correctly"
    - "Arrow allocators are released: no cursor accumulation after N checkout/checkin cycles with unclosed cursors"
    - "Importing adbc_poolhouse creates no pool or connection object at module level"
    - "create_pool, PoolhouseError, ConfigurationError are importable from adbc_poolhouse"
    - "DuckDBConfig raises ConfigurationError for pool_size <= 0, max_overflow < 0, timeout <= 0, recycle <= 0, database empty"
    - "prek passes with no violations"
  artifacts:
    - path: "tests/test_pool_factory.py"
      provides: "Integration tests for POOL-01..05, TEST-01, TEST-07"
      contains: "TestCreatePoolDuckDB"
    - path: "src/adbc_poolhouse/_pool_factory.py"
      provides: "create_pool() factory function"
      contains: "def create_pool"
    - path: "src/adbc_poolhouse/__init__.py"
      provides: "Public re-exports including create_pool, PoolhouseError, ConfigurationError"
      contains: "create_pool"
  key_links:
    - from: "src/adbc_poolhouse/_pool_factory.py"
      to: "src/adbc_poolhouse/_driver_api.py"
      via: "from adbc_poolhouse._driver_api import create_adbc_connection"
      pattern: "create_adbc_connection"
    - from: "src/adbc_poolhouse/_pool_factory.py"
      to: "src/adbc_poolhouse/_drivers.py"
      via: "from adbc_poolhouse._drivers import resolve_driver"
      pattern: "resolve_driver"
    - from: "src/adbc_poolhouse/_pool_factory.py"
      to: "src/adbc_poolhouse/_translators.py"
      via: "from adbc_poolhouse._translators import translate_config"
      pattern: "translate_config"
    - from: "tests/test_pool_factory.py"
      to: "src/adbc_poolhouse/_pool_factory.py"
      via: "from adbc_poolhouse import create_pool"
      pattern: "create_pool"
---

<objective>
Implement `create_pool()` via TDD — write the test file first (RED), then implement the factory (GREEN), then update `__init__.py` to export the new public API.

Purpose: This is the primary deliverable of Phase 5 — the complete public pool factory that consumers call with a config to get a ready-to-use QueuePool.
Output: `tests/test_pool_factory.py` (full integration test suite), `src/adbc_poolhouse/_pool_factory.py` (factory implementation), updated `__init__.py` (new public exports).
</objective>

<execution_context>
@/Users/paul/.claude/get-shit-done/workflows/execute-plan.md
@/Users/paul/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-pool-factory-and-duckdb-integration/05-CONTEXT.md
@.planning/phases/05-pool-factory-and-duckdb-integration/05-RESEARCH.md
@.planning/phases/05-pool-factory-and-duckdb-integration/05-01-SUMMARY.md
@src/adbc_poolhouse/_base_config.py
@src/adbc_poolhouse/_duckdb_config.py
@src/adbc_poolhouse/_exceptions.py
@src/adbc_poolhouse/_driver_api.py
@src/adbc_poolhouse/_drivers.py
@src/adbc_poolhouse/_translators.py
@src/adbc_poolhouse/__init__.py
</context>

<feature>
  <name>create_pool() — SQLAlchemy QueuePool factory for ADBC warehouse configs</name>
  <files>tests/test_pool_factory.py, src/adbc_poolhouse/_pool_factory.py, src/adbc_poolhouse/__init__.py</files>
  <behavior>
    Given any supported WarehouseConfig instance, create_pool() returns a sqlalchemy.pool.QueuePool.

    Cases:
    - create_pool(DuckDBConfig(database=path)) → QueuePool instance
    - create_pool(cfg) has pool_size=5, max_overflow=3, timeout=30, pre_ping=False, recycle=3600 by default
    - create_pool(cfg, pool_size=10, recycle=7200) → QueuePool with pool.size()==10
    - pool.connect() returns a connection; cursor executes SELECT; checkin works; pool.dispose() cleans up
    - N checkout/checkin cycles with unclosed cursors: no cursor accumulation (reset event fires)
    - import adbc_poolhouse creates no pool or connection at module level
    - create_pool, PoolhouseError, ConfigurationError importable from top-level adbc_poolhouse
    - DuckDBConfig(pool_size=0) raises ValidationError (ConfigurationError inside, message includes "got 0")
    - DuckDBConfig(pool_size=-1) raises ValidationError (ConfigurationError inside, message includes "got -1")
    - DuckDBConfig(max_overflow=-1) raises ValidationError (ConfigurationError inside)
    - DuckDBConfig(timeout=0) raises ValidationError (ConfigurationError inside)
    - DuckDBConfig(recycle=0) raises ValidationError (ConfigurationError inside)
    - DuckDBConfig(database='') raises ValidationError (ConfigurationError inside)
  </behavior>
  <implementation>
    Use the ADBC source+clone pattern from RESEARCH.md:
    1. resolve_driver(config) → driver_path
    2. translate_config(config) → kwargs dict
    3. config._adbc_entrypoint() → entrypoint (None for all except DuckDB)
    4. create_adbc_connection(driver_path, kwargs, entrypoint=entrypoint) → source connection
    5. QueuePool(source.adbc_clone, pool_size=..., ...) → pool
    6. pool._adbc_source = source (keep source alive)
    7. event.listen(pool, 'reset', _release_arrow_allocators)
    8. return pool

    Arrow cleanup listener iterates source._cursors (WeakSet), closes unclosed cursors via cur.close().
  </implementation>
</feature>

<tasks>

<task type="auto">
  <name>RED: Write test_pool_factory.py (all tests fail — implementation does not exist yet)</name>
  <files>tests/test_pool_factory.py</files>
  <action>
Create `tests/test_pool_factory.py` with the complete test suite. Tests import `create_pool` from `adbc_poolhouse` — this import will fail until implementation exists (RED state is expected).

```python
"""Integration tests for create_pool() factory (POOL-01..05, TEST-01, TEST-07)."""

import importlib

import pytest
import sqlalchemy.pool

from adbc_poolhouse import ConfigurationError, DuckDBConfig, PoolhouseError, create_pool


class TestCreatePoolDuckDB:
    """POOL-01, POOL-02, POOL-03, TEST-01."""

    def test_create_pool_returns_queuepool(self, tmp_path: pytest.TempPathFactory) -> None:
        """POOL-01: create_pool returns a QueuePool instance."""
        cfg = DuckDBConfig(database=str(tmp_path / "test.db"))
        pool = create_pool(cfg)
        try:
            assert isinstance(pool, sqlalchemy.pool.QueuePool)
        finally:
            pool.dispose()
            pool._adbc_source.close()

    def test_default_pool_settings(self, tmp_path: pytest.TempPathFactory) -> None:
        """POOL-02: default pool settings are pool_size=5, max_overflow=3, timeout=30."""
        cfg = DuckDBConfig(database=str(tmp_path / "test.db"))
        pool = create_pool(cfg)
        try:
            assert pool.size() == 5
            assert pool._max_overflow == 3
            assert pool._timeout == 30
        finally:
            pool.dispose()
            pool._adbc_source.close()

    def test_pool_size_override(self, tmp_path: pytest.TempPathFactory) -> None:
        """POOL-03: create_pool kwargs override defaults."""
        cfg = DuckDBConfig(database=str(tmp_path / "test.db"))
        pool = create_pool(cfg, pool_size=10, recycle=7200)
        try:
            assert pool.size() == 10
        finally:
            pool.dispose()
            pool._adbc_source.close()

    def test_checkout_query_checkin_dispose(self, tmp_path: pytest.TempPathFactory) -> None:
        """TEST-01: full lifecycle — checkout, query, checkin, dispose."""
        cfg = DuckDBConfig(database=str(tmp_path / "test.db"))
        pool = create_pool(cfg)
        try:
            conn = pool.connect()
            cur = conn.cursor()
            cur.execute("SELECT 42 AS answer")
            row = cur.fetchone()
            assert row == (42,)
            cur.close()
            conn.close()
            assert pool.checkedin() == 1
        finally:
            pool.dispose()
            pool._adbc_source.close()


class TestArrowAllocatorCleanup:
    """POOL-04, TEST-07: Arrow allocator cleanup via reset event."""

    def test_no_cursor_accumulation_after_checkin_cycles(
        self, tmp_path: pytest.TempPathFactory
    ) -> None:
        """TEST-07: N checkout/checkin cycles with unclosed cursors — no accumulation."""
        cfg = DuckDBConfig(database=str(tmp_path / "leak_test.db"))
        pool = create_pool(cfg)
        try:
            n = 10
            for i in range(n):
                conn = pool.connect()
                cur = conn.cursor()
                cur.execute(f"SELECT {i} AS val")
                # Intentionally do NOT close cursor before checkin.
                # reset event listener must close it on return.
                conn.close()

            # Pool must still function after N unclosed-cursor cycles.
            conn = pool.connect()
            cur = conn.cursor()
            cur.execute("SELECT 999")
            assert cur.fetchone() == (999,)
            cur.close()
            conn.close()
        finally:
            pool.dispose()
            pool._adbc_source.close()


class TestNoGlobalState:
    """POOL-05: importing adbc_poolhouse creates no pool or connection."""

    def test_import_creates_no_pool_or_connection(self) -> None:
        """POOL-05: import adbc_poolhouse produces no module-level pool/connection."""
        import adbc_poolhouse

        # Inspect module namespace — no QueuePool or Connection attributes
        for name in dir(adbc_poolhouse):
            val = getattr(adbc_poolhouse, name)
            assert not isinstance(
                val, sqlalchemy.pool.QueuePool
            ), f"Module-level QueuePool found: {name}"

    def test_reimport_creates_no_pool(self) -> None:
        """POOL-05: re-importing module does not create pools."""
        importlib.reload(importlib.import_module("adbc_poolhouse"))
        import adbc_poolhouse

        for name in dir(adbc_poolhouse):
            val = getattr(adbc_poolhouse, name)
            assert not isinstance(val, sqlalchemy.pool.QueuePool)


class TestExceptionHierarchy:
    """Exception hierarchy: PoolhouseError, ConfigurationError, and DuckDBConfig bounds validators."""

    def test_poolhouse_error_importable(self) -> None:
        assert issubclass(PoolhouseError, Exception)

    def test_configuration_error_hierarchy(self) -> None:
        assert issubclass(ConfigurationError, PoolhouseError)
        assert issubclass(ConfigurationError, ValueError)

    def test_duckdb_memory_raises_configuration_error(self) -> None:
        """TEST-02: DuckDBConfig(:memory:, pool_size=2) raises ValidationError (ConfigurationError inside)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError):
            DuckDBConfig(database=":memory:", pool_size=2)

    def test_pool_size_zero_raises_configuration_error(self) -> None:
        """pool_size=0 is rejected (must be > 0)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError) as exc_info:
            DuckDBConfig(pool_size=0)
        assert "pool_size" in str(exc_info.value)
        assert "0" in str(exc_info.value)

    def test_pool_size_negative_raises_configuration_error(self) -> None:
        """pool_size=-1 is rejected (must be > 0)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError) as exc_info:
            DuckDBConfig(pool_size=-1)
        assert "pool_size" in str(exc_info.value)
        assert "-1" in str(exc_info.value)

    def test_max_overflow_negative_raises_configuration_error(self) -> None:
        """max_overflow=-1 is rejected (must be >= 0)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError) as exc_info:
            DuckDBConfig(max_overflow=-1)
        assert "max_overflow" in str(exc_info.value)
        assert "-1" in str(exc_info.value)

    def test_timeout_zero_raises_configuration_error(self) -> None:
        """timeout=0 is rejected (must be > 0)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError) as exc_info:
            DuckDBConfig(timeout=0)
        assert "timeout" in str(exc_info.value)
        assert "0" in str(exc_info.value)

    def test_recycle_zero_raises_configuration_error(self) -> None:
        """recycle=0 is rejected (must be > 0)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError) as exc_info:
            DuckDBConfig(recycle=0)
        assert "recycle" in str(exc_info.value)
        assert "0" in str(exc_info.value)

    def test_database_empty_string_raises_configuration_error(self) -> None:
        """database='' is rejected (must be a non-empty string)."""
        from pydantic import ValidationError

        with pytest.raises(ValidationError):
            DuckDBConfig(database="")
```

After creating the file, run tests to confirm they FAIL (RED state):
`uv run pytest tests/test_pool_factory.py -x --tb=short 2>&1 | tail -20`

Expected: ImportError on `from adbc_poolhouse import create_pool` (create_pool not yet exported).

Commit: `test(05-02): add failing tests for create_pool factory (RED)`
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && uv run pytest tests/test_pool_factory.py --tb=short 2>&1 | grep -E "(FAILED|ERROR|ImportError|passed|failed)" | head -10</automated>
    <manual>Confirm tests fail due to missing create_pool import — not due to syntax errors in the test file itself.</manual>
  </verify>
  <done>tests/test_pool_factory.py exists with all test classes including the five bounds validator tests; running it produces failures/errors (not passes) because create_pool is not yet implemented.</done>
</task>

<task type="auto">
  <name>GREEN: Implement _pool_factory.py and update __init__.py</name>
  <files>src/adbc_poolhouse/_pool_factory.py, src/adbc_poolhouse/__init__.py</files>
  <action>
**Step 1: Create `src/adbc_poolhouse/_pool_factory.py`**

```python
"""
Pool factory: public create_pool() entry point.

This module wires together config translation, driver resolution, and
the ADBC source+clone pattern to create a SQLAlchemy QueuePool.

No module-level pool or connection objects exist here (POOL-05).

Internal helpers are prefixed with _ and not exported.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import sqlalchemy.pool
from sqlalchemy import event

from adbc_poolhouse._driver_api import create_adbc_connection
from adbc_poolhouse._drivers import resolve_driver
from adbc_poolhouse._translators import translate_config

if TYPE_CHECKING:
    from adbc_poolhouse._base_config import WarehouseConfig


def create_pool(
    config: WarehouseConfig,
    *,
    pool_size: int = 5,
    max_overflow: int = 3,
    timeout: int = 30,
    recycle: int = 3600,
    pre_ping: bool = False,
) -> sqlalchemy.pool.QueuePool:
    """
    Create a SQLAlchemy QueuePool backed by an ADBC warehouse driver.

    Accepts any supported warehouse config model and returns a ready-to-use
    ``sqlalchemy.pool.QueuePool``. The pool uses the official ADBC
    ``adbc_clone`` pattern: one source connection is created, and each pool
    checkout calls ``source.adbc_clone()`` to open a new connection sharing
    the same underlying ``AdbcDatabase`` via reference counting.

    An Arrow allocator cleanup listener is registered on the pool's ``reset``
    event. Any cursors left open when a connection is returned to the pool are
    closed automatically, releasing Arrow record batch reader memory (POOL-04).

    The source connection is attached to the pool as ``pool._adbc_source``.
    Callers must close it after ``pool.dispose()``::

        pool.dispose()
        pool._adbc_source.close()

    Args:
        config: A warehouse config model instance (e.g. ``DuckDBConfig``).
        pool_size: Number of connections to keep in the pool. Default: 5.
        max_overflow: Extra connections allowed above pool_size. Default: 3.
        timeout: Seconds to wait for a connection before raising. Default: 30.
        recycle: Seconds before a connection is recycled. Default: 3600.
        pre_ping: Whether to ping connections before checkout. Default: False.
            Pre-ping does not function on a standalone QueuePool without a
            SQLAlchemy dialect; recycle is the preferred health mechanism.

    Returns:
        A configured ``sqlalchemy.pool.QueuePool`` ready for use.

    Raises:
        ImportError: If the required ADBC driver is not installed.
        TypeError: If ``config`` is not a recognised warehouse config type.
    """
    driver_path = resolve_driver(config)
    kwargs = translate_config(config)
    entrypoint = config._adbc_entrypoint()

    source = create_adbc_connection(driver_path, kwargs, entrypoint=entrypoint)

    pool = sqlalchemy.pool.QueuePool(
        source.adbc_clone,  # type: ignore[arg-type]
        pool_size=pool_size,
        max_overflow=max_overflow,
        timeout=timeout,
        recycle=recycle,
        pre_ping=pre_ping,
    )

    pool._adbc_source = source  # type: ignore[attr-defined]

    event.listen(pool, "reset", _release_arrow_allocators)

    return pool


def _release_arrow_allocators(
    dbapi_conn: object,
    connection_record: object,
    reset_state: object,
) -> None:
    """
    Close any open cursors to release Arrow record batch readers.

    Registered on the pool ``reset`` event, which fires on all connection
    return paths: normal checkin, invalidation, and error. The ``reset``
    event is preferred over ``checkin`` because ``checkin`` receives
    ``None`` as ``dbapi_conn`` when the connection is invalidated.

    ADBC connections store open cursors in ``_cursors: weakref.WeakSet``.
    Iterating and closing them releases Arrow streaming memory.
    """
    if dbapi_conn is None:
        return
    for cur in list(getattr(dbapi_conn, "_cursors", [])):
        if not getattr(cur, "_closed", True):
            try:
                cur.close()
            except Exception:  # noqa: BLE001
                pass
```

**Step 2: Update `src/adbc_poolhouse/__init__.py`**

Add imports for `create_pool`, `PoolhouseError`, `ConfigurationError` and add them to `__all__`:

```python
"""Connection pooling for ADBC drivers from typed warehouse configs."""

from adbc_poolhouse._base_config import BaseWarehouseConfig, WarehouseConfig
from adbc_poolhouse._bigquery_config import BigQueryConfig
from adbc_poolhouse._databricks_config import DatabricksConfig
from adbc_poolhouse._duckdb_config import DuckDBConfig
from adbc_poolhouse._exceptions import ConfigurationError, PoolhouseError
from adbc_poolhouse._flightsql_config import FlightSQLConfig
from adbc_poolhouse._mssql_config import MSSQLConfig
from adbc_poolhouse._pool_factory import create_pool
from adbc_poolhouse._postgresql_config import PostgreSQLConfig
from adbc_poolhouse._redshift_config import RedshiftConfig
from adbc_poolhouse._snowflake_config import SnowflakeConfig
from adbc_poolhouse._teradata_config import TeradataConfig
from adbc_poolhouse._trino_config import TrinoConfig

__all__ = [
    "ConfigurationError",
    "PoolhouseError",
    "WarehouseConfig",
    "BaseWarehouseConfig",
    "BigQueryConfig",
    "DatabricksConfig",
    "DuckDBConfig",
    "FlightSQLConfig",
    "MSSQLConfig",
    "PostgreSQLConfig",
    "RedshiftConfig",
    "SnowflakeConfig",
    "TeradataConfig",
    "TrinoConfig",
    "create_pool",
]
```

Note: `create_pool` and exception imports are NOT in a TYPE_CHECKING block — they are public API and must be importable at runtime.

After both files are created/updated, run prek:
`uv run prek --hook-stage manual 2>&1 | tail -10`

If ruff flags `_pool_factory.py` imports as TC001 (WarehouseConfig only used in TYPE_CHECKING), the `if TYPE_CHECKING:` block in `_pool_factory.py` already handles this correctly. If ruff flags anything else, resolve before committing.

Commit: `feat(05-02): implement create_pool factory with Arrow cleanup and public exports (GREEN)`
  </action>
  <verify>
    <automated>cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && uv run pytest tests/test_pool_factory.py -x --tb=short</automated>
  </verify>
  <done>All tests in test_pool_factory.py pass including TestExceptionHierarchy bounds validator tests; uv run pytest tests/ -x passes (no regressions); prek passes with zero violations.</done>
</task>

</tasks>

<verification>
Run full test suite:
`cd /Users/paul/Documents/Dev/Personal/adbc-poolhouse && uv run pytest tests/ -x --tb=short`

All tests must pass including:
- `tests/test_configs.py` — no regressions from ConfigurationError change
- `tests/test_translators.py` — no regressions
- `tests/test_drivers.py` — no regressions
- `tests/test_pool_factory.py` — all new pool factory tests green

Verify public API:
`uv run python -c "from adbc_poolhouse import create_pool, PoolhouseError, ConfigurationError, DuckDBConfig; print('OK')`
</verification>

<success_criteria>
- `tests/test_pool_factory.py` passes all test classes: TestCreatePoolDuckDB, TestArrowAllocatorCleanup, TestNoGlobalState, TestExceptionHierarchy
- `create_pool(DuckDBConfig(database=tmp_path/'test.db'))` returns `sqlalchemy.pool.QueuePool`
- Default pool settings: pool_size=5, max_overflow=3, timeout=30
- Pool size override works: `create_pool(cfg, pool_size=10)` sets pool.size()==10
- N checkout/checkin cycles with unclosed cursors: pool remains healthy (no accumulation)
- `import adbc_poolhouse` produces no QueuePool or connection in module namespace
- `from adbc_poolhouse import create_pool, PoolhouseError, ConfigurationError` succeeds
- Bounds validator tests pass: pool_size=0, pool_size=-1, max_overflow=-1, timeout=0, recycle=0, database='' all raise ValidationError
- Full suite green: `uv run pytest tests/ -x`
- prek passes
</success_criteria>

<output>
After completion, create `.planning/phases/05-pool-factory-and-duckdb-integration/05-02-SUMMARY.md`
</output>
